---
title: "Multi-Granularity Contrasting for Cross-Lingual Pre-Training"
collection: publications
permalink: /publication/mgc
excerpt: 'Cross-lingual pre-training aims at providing effective prior representations for the inputs from multiple languages. With the modeling of bidirectional contexts, recently prevalent language modeling approaches such as XLM achieve better performance than traditional methods based on embedding alignment, which strives to assign similar vector representations to semantic-equivalent units. However, such approaches like XLM capture cross-lingual information based solely on shared BPE vocabulary, resulting in the absence of ﬁne-grained supervision induced by embedding alignment. Inheriting the advantages of the above two paradigms, this work presents a multi-granularity contrasting framework, namely MGC, to learn language-universal representations. While predicting the masked words based on bidirectional contexts, the proposal also encodes semantic equivalents from different languages into similar representations to introduce more ﬁne-grained and explicit cross-lingual information. Two effective contrasting strategies are further proposed, which can be built upon semantic units of multiple granularities covering words, span, and sentences. Extensive experiments demonstrate that our approach can achieve signiﬁcant performance gains in various down-stream tasks, including machine translation and cross-lingual language understanding.'
venue: 'Findings of ACL 2021'
paperurl: 'https://aclanthology.org/2021.findings-acl.149.pdf'
authors: '<b>Shicheng Li</b>, Fuli Luo, Pengcheng Yang, Jun Xie'
---
